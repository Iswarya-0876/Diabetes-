# -*- coding: utf-8 -*-
"""project.diabetes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ygTjud9K0GSZWhLW7qv65AWyR_KgiM8-
"""

from google.colab import drive
drive.mount('/gdrive')

import os
os.chdir('/gdrive/My Drive')

import pandas as pd # manipulate your data
D = pd.read_csv("Diabetes.csv", index_col=0)
print(f"Data has {D.shape[0]} rows and {D.shape[1]} columns.\n")
D.head()

D.info()

D.describe()

D.isnull().sum()

D.shape

D.columns

D.corr()

D['Outcome'].value_counts()

D.groupby('Outcome').mean()

D.groupby('Outcome').median()

D.groupby('Outcome').std()

D['Glucose'].value_counts()

D.plot(kind='box',subplots=True,layout=(3,3),sharex=False,figsize=(10,10))

iso = IsolationForest(contamination=0.1)
yhat = iso.fit_predict(D.drop('Outcome',axis=1))

mask = yhat != -1
data_no_outliers = D[mask]

print("Original Data Summary:")
print(D.describe())

print("\nDta without Outliers Summary:")
print(data_no_outliers.describe())

for column in D.columns[:-1]:
    q1 = D[column].quantile(0.25)
    q3 = D[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    D[column] = np.where(D[column] < lower_bound, lower_bound, D[column])
    D[column] = np.where(D[column] > upper_bound, upper_bound, D[column])

print("\nData with Capped Outliers Summary:")
print(D.describe())

D.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(10,10))

D.hist(figsize=(10,10), color = "Purple")

D.head()

plt.figure(figsize=(8, 6))
sns.scatterplot(x='BloodPressure', y='Insulin', hue='Outcome', data=D, palette='viridis')
plt.title('Blood Pressure vs. Insulin (Colored by Outcome)')
plt.xlabel('Blood Pressure')
plt.ylabel('Insulin')
plt.show()

sns.pairplot(D, hue='Outcome')
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(D.corr(), annot=True, cmap='coolwarm')
plt.show()

X = D.drop('Outcome', axis=1)
y = D['Outcome']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(probability=True),
    'KNN': KNeighborsClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100)
}

model_name = 'Logistic Regression'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_name = 'Decision Tree'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greys', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_name = 'Random Forest'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_name = 'SVM'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_name = 'KNN'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_name = 'Gradient Boosting'
model = models[model_name]

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
print(f"{model_name} Accuracy: {accuracy:.2f}")

print(f"Classification Report for {model_name}:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve for {model_name}')
plt.legend(loc="lower right")
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve for {model_name}')
plt.legend(loc="lower left")
plt.show()

model_comparison = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': [],
    'AUC-ROC': []
}

for model_name, model in models.items():
    print(f"Training and evaluating {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else [0] * len(y_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba)

    model_comparison['Model'].append(model_name)
    model_comparison['Accuracy'].append(accuracy)
    model_comparison['Precision'].append(precision)
    model_comparison['Recall'].append(recall)
    model_comparison['F1-Score'].append(f1)
    model_comparison['AUC-ROC'].append(auc_roc)

comparison_df = pd.DataFrame(model_comparison)

print("Model Comparison Table:")
print(comparison_df)

plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
sns.barplot(x='Model', y='Accuracy', data=comparison_df, palette='viridis')
plt.title('Accuracy Comparison')
plt.xticks(rotation=45)

plt.subplot(2, 2, 2)
sns.barplot(x='Model', y='Precision', data=comparison_df, palette='viridis')
plt.title('Precision Comparison')
plt.xticks(rotation=45)

sns.barplot(x='Model', y='Recall', data=comparison_df, palette='viridis')
plt.title('Recall Comparison')
plt.xticks(rotation=45)

sns.barplot(x='Model', y='F1-Score', data=comparison_df, palette='viridis')
plt.title('F1-Score Comparison')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.barplot(x='Model', y='AUC-ROC', data=comparison_df, palette='viridis')
plt.title('AUC-ROC Comparison')
plt.xticks(rotation=45)
plt.show()

def is_diabetic(glucose, insulin):

    # Criteria for diabetes diagnosis
    # Fasting glucose level of 126 mg/dL or higher indicates diabetes
    if glucose >= 126:
        return "This person is diabetic based on glucose levels."

    # Insulin resistance can be indicated by high insulin levels
    # A common threshold for insulin resistance is > 25 µU/mL
    if insulin > 25:
        return "This person may be insulin resistant, indicating potential diabetes."

    return "This person is not diabetic based on the provided values."

# Example usage
glucose = 130  # Example fasting glucose level in mg/dL
insulin = 30           # Example insulin level in µU/mL

result = is_diabetic(glucose, insulin)
print(result)
